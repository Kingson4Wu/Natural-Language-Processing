>> 以下内容由ChatGPT辅助生成 
>> 2025-02-10


# Word2Vec 中的 Embedding 层 和 Negative Sampling 的损失函数

### 1. **Embedding 层**（嵌入层）

在 Word2Vec 中，Embedding 层用于将单词转换为密集向量（低维向量）。在传统的 NLP 方法中，单词通常被表示为独热（one-hot）向量，维度非常高且稀疏。例如：

- 词汇表：`["apple", "banana", "cat", "dog"]`
- "banana" 的独热表示：`[0, 1, 0, 0]`（假设词汇表大小为 4）

然而，这种表示方式有很多缺点，比如维度太高且不能表示单词之间的关系。因此，Word2Vec 通过 **Embedding 层** 来学习一个更低维度的、密集的、具有语义信息的向量。例如，Embedding 层会将单词映射到一个 100 维的向量空间：

- "banana" → `[0.3, -0.2, 0.7, ..., 0.1]`（100 维）

在实现上，Embedding 层本质上是一个查找表，每个单词都有一个可训练的向量。训练过程中，Embedding 层的参数会不断更新，使得相似语义的单词向量靠得更近。

### 2. **Negative Sampling（负采样）**

负采样是一种用于训练 Word2Vec（特别是 Skip-Gram 模型）的损失函数优化方法。它的目的是减少计算复杂度，同时让模型更高效地学习有用的词向量。

#### **问题背景：Softmax 的计算开销**
在标准的 Word2Vec 训练中，我们通常使用 **Softmax** 计算目标单词的概率：

\[
P(w_O | w_I) = \frac{\exp(v_{w_O} \cdot v_{w_I})}{\sum_{w \in V} \exp(v_w \cdot v_{w_I})}
\]

其中：
- \( w_I \) 是中心词（输入词）
- \( w_O \) 是上下文词（目标词）
- \( v_w \) 是词向量
- \( V \) 是整个词汇表

由于分母需要计算整个词汇表的所有单词的指数和，对于大规模词汇（如百万级别）来说，计算量非常大。

#### **Negative Sampling 解决方案**
负采样的核心思想是：
1. 只优化一小部分 **负样本**（negative samples），而不是计算整个词汇表。
2. 正样本（Positive Sample）：正确的上下文单词（即真实的 \( w_O \)）。
3. 负样本（Negative Sample）：从词汇表中随机采样的一些错误单词（不是上下文）。

在训练时，模型只需计算 **一个正样本** 和 **几个负样本**（通常 5~20 个），大大降低计算成本。

#### **损失函数**
负采样的目标是让正样本的相似度更高，同时让负样本的相似度降低。损失函数如下：

\[
J = \log(\sigma(v_{w_O} \cdot v_{w_I})) + \sum_{k=1}^{K} \log(\sigma(- v_{w_k} \cdot v_{w_I}))
\]

其中：
- \( \sigma(x) \) 是 Sigmoid 函数
- \( w_O \) 是正样本
- \( w_k \) 是负样本
- \( K \) 是负样本的数量

这个优化目标让模型：
- **增大** 正确的上下文单词的相似度（通过 Sigmoid 使得概率接近 1）
- **减小** 负样本的相似度（通过 Sigmoid 使得概率接近 0）

### **总结**
1. **Embedding 层**：将独热向量转换为低维、密集的词向量，使模型可以高效学习语义信息。
2. **Negative Sampling**：通过仅计算一部分负样本，而不是整个词汇表，提高 Word2Vec 训练效率。

这两个改进共同作用，使得 Word2Vec 在大规模数据上训练更快，同时得到更好的词向量表示。

---

# 大语言模型在输出结果的随机性

已经训练完成的大语言模型在输出结果时通常具有一定的随机性。这种随机性主要来自以下几个方面：  

1. **采样方法**  
   生成文本时，模型通常会使用采样策略，而不同的采样策略会影响输出的多样性：
   - **贪心搜索（Greedy Search）**：每次选择概率最高的词，确定性强，但容易陷入重复和单调的文本。
   - **束搜索（Beam Search）**：在多个候选序列中搜索最优解，可以改善连贯性，但仍可能导致低多样性。
   - **温度（Temperature）控制**：调节词的概率分布，高温度（如 1.5）使输出更随机，低温度（如 0.2）则更确定。
   - **Top-k 采样**：只在前 k 个最可能的词中进行采样，避免低概率词的干扰，提高流畅度。
   - **Top-p（核采样）**：只从累计概率达到 p（如 0.9）的词中抽取，提高灵活性。

2. **非确定性机制**  
   训练好的模型在推理过程中，若未固定随机种子（random seed），即使面对相同输入，每次生成的内容可能都不同。这是因为深度学习框架（如 PyTorch、TensorFlow）在执行计算时可能涉及浮点数精度误差、硬件差异等因素。

3. **上下文依赖性**  
   语言模型在生成文本时会根据当前的上下文动态调整输出，稍微不同的输入可能导致完全不同的结果。这种**蝴蝶效应**在长文本生成时尤为明显。

4. **多轮对话的累积影响**  
   在对话系统中，模型的输出受到历史对话的影响，即使是相同的初始问题，不同的对话路径可能导致不同的回答。

### 如何减少随机性？
如果希望模型的输出更稳定，可以采取以下措施：
- **使用固定的采样策略**（如 Greedy Search 或低温度的 Top-k/Top-p 采样）。
- **设定随机种子（Random Seed）**，确保多次生成的结果一致。
- **减少外部干扰**，比如固定使用相同的计算设备和环境（但这仍无法完全消除随机性）。

总之，大语言模型的输出具有一定的随机性，但这种随机性可以通过调整采样策略和设置随机种子来减少。

----
